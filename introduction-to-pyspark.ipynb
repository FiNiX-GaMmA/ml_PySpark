{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502c7493",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-28T05:01:49.264367Z",
     "iopub.status.busy": "2023-11-28T05:01:49.263930Z",
     "iopub.status.idle": "2023-11-28T05:01:49.538402Z",
     "shell.execute_reply": "2023-11-28T05:01:49.537762Z",
     "shell.execute_reply.started": "2023-11-28T05:01:49.264339Z"
    },
    "papermill": {
     "duration": 0.010416,
     "end_time": "2023-11-28T07:26:48.403766",
     "exception": false,
     "start_time": "2023-11-28T07:26:48.393350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic understanding of PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ac5c2",
   "metadata": {
    "papermill": {
     "duration": 0.007907,
     "end_time": "2023-11-28T07:26:48.419935",
     "exception": false,
     "start_time": "2023-11-28T07:26:48.412028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Installing the latest pyspark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7caa121b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:26:48.438497Z",
     "iopub.status.busy": "2023-11-28T07:26:48.437592Z",
     "iopub.status.idle": "2023-11-28T07:27:43.424712Z",
     "shell.execute_reply": "2023-11-28T07:27:43.423269Z"
    },
    "papermill": {
     "duration": 54.999575,
     "end_time": "2023-11-28T07:27:43.427807",
     "exception": false,
     "start_time": "2023-11-28T07:26:48.428232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=7572c5392a8a2334b0da1abf5e67da1cf0728df81aeb062508cf5cc2f5ae681d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977f149",
   "metadata": {
    "papermill": {
     "duration": 0.020115,
     "end_time": "2023-11-28T07:27:43.468600",
     "exception": false,
     "start_time": "2023-11-28T07:27:43.448485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing the PySpark to work with the modules\n",
    "\n",
    "- `SparkContext` is the _entry point_ to any <u>Spark functionality</u>. It represents the connection to a Spark cluster and can be used to create **RDDs** (Resilient Distributed Datasets) and broadcast variables on that cluster.\n",
    "\n",
    "- `SparkSession` is the _entry point_ to use DataFrame and SQL functionality in Spark. It provides a way to interact with various data sources (like Parquet, Avro, JSON, etc.) in a tabular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f800cf89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:43.511809Z",
     "iopub.status.busy": "2023-11-28T07:27:43.511391Z",
     "iopub.status.idle": "2023-11-28T07:27:43.609499Z",
     "shell.execute_reply": "2023-11-28T07:27:43.608161Z"
    },
    "papermill": {
     "duration": 0.123045,
     "end_time": "2023-11-28T07:27:43.612492",
     "exception": false,
     "start_time": "2023-11-28T07:27:43.489447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78d8a5",
   "metadata": {
    "papermill": {
     "duration": 0.020208,
     "end_time": "2023-11-28T07:27:43.653773",
     "exception": false,
     "start_time": "2023-11-28T07:27:43.633565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. `SparkContext.getOrCreate()` tries to get an existing SparkContext or creates a new one if it doesn't exist. The SparkConf().setMaster(\"local[*]\") part specifies that Spark should run in **local mode** using all <u>available CPU cores ([*])</u>. In local mode, Spark runs on a single machine, which is useful for development and testing.\n",
    "2. `SparkSession.builder.getOrCreate()` follows a similar pattern as `SparkContext.getOrCreate()`. It tries to get an existing SparkSession or creates a new one if it doesn't exist. The `SparkSession` is the <u>unified entry point for reading data, executing SQL queries, and performing other Spark operations using the DataFrame API.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce65bb6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:43.698468Z",
     "iopub.status.busy": "2023-11-28T07:27:43.698019Z",
     "iopub.status.idle": "2023-11-28T07:27:49.981473Z",
     "shell.execute_reply": "2023-11-28T07:27:49.980403Z"
    },
    "papermill": {
     "duration": 6.309611,
     "end_time": "2023-11-28T07:27:49.984827",
     "exception": false,
     "start_time": "2023-11-28T07:27:43.675216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/28 07:27:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkContext if it doesn't exist, using a local master URL\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "# Create a SparkSession if it doesn't exist\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45c8b3",
   "metadata": {
    "papermill": {
     "duration": 0.020034,
     "end_time": "2023-11-28T07:27:50.025672",
     "exception": false,
     "start_time": "2023-11-28T07:27:50.005638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating an RDD (Resilient Distributed Dataset) using the parallelize method of the SparkContext (sc). Let me break down what this line does:\n",
    "\n",
    "   - `range(100)`: This creates a Python list containing numbers from 0 to 99.\n",
    "\n",
    "   - `sc.parallelize(range(100))`: This takes that Python list and distributes it across the Spark cluster, creating an RDD. The RDD is a fault-tolerant collection of elements that can be processed in parallel.\n",
    "\n",
    "So, the rdd variable now represents a distributed collection of numbers from 0 to 99. Each element of the RDD is stored on a different partition of the cluster, and Spark can perform operations on these partitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715c2111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:50.068424Z",
     "iopub.status.busy": "2023-11-28T07:27:50.067983Z",
     "iopub.status.idle": "2023-11-28T07:27:50.481092Z",
     "shell.execute_reply": "2023-11-28T07:27:50.479820Z"
    },
    "papermill": {
     "duration": 0.437761,
     "end_time": "2023-11-28T07:27:50.484130",
     "exception": false,
     "start_time": "2023-11-28T07:27:50.046369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fef72",
   "metadata": {
    "papermill": {
     "duration": 0.020309,
     "end_time": "2023-11-28T07:27:50.525226",
     "exception": false,
     "start_time": "2023-11-28T07:27:50.504917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `take` action in Spark is used to retrieve the **first N elements** from an RDD or DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25c8b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:50.568857Z",
     "iopub.status.busy": "2023-11-28T07:27:50.568444Z",
     "iopub.status.idle": "2023-11-28T07:27:52.936725Z",
     "shell.execute_reply": "2023-11-28T07:27:52.935331Z"
    },
    "papermill": {
     "duration": 2.393974,
     "end_time": "2023-11-28T07:27:52.939887",
     "exception": false,
     "start_time": "2023-11-28T07:27:50.545913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10) #note the as an array the rdd contains number from 0 to (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfb434",
   "metadata": {
    "papermill": {
     "duration": 0.021212,
     "end_time": "2023-11-28T07:27:52.985324",
     "exception": false,
     "start_time": "2023-11-28T07:27:52.964112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functional Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36bad0",
   "metadata": {
    "papermill": {
     "duration": 0.020838,
     "end_time": "2023-11-28T07:27:53.029061",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.008223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A python function which decides whether a value is greater than 50 (**True**) or not (**False**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a9ff31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:53.140476Z",
     "iopub.status.busy": "2023-11-28T07:27:53.140051Z",
     "iopub.status.idle": "2023-11-28T07:27:53.146204Z",
     "shell.execute_reply": "2023-11-28T07:27:53.144943Z"
    },
    "papermill": {
     "duration": 0.101375,
     "end_time": "2023-11-28T07:27:53.151451",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.050076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def gt50(i):\n",
    "    return i > 50\n",
    "\n",
    "print(gt50(4))\n",
    "print(gt50(51))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3b2f2",
   "metadata": {
    "papermill": {
     "duration": 0.026601,
     "end_time": "2023-11-28T07:27:53.207363",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.180762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Writing the above function but by lambda function\n",
    "\n",
    "This line of code defines a lambda function (lambda) named gt25. The lambda function takes a single argument `i` and returns a _Boolean value (True or False)_. In this case, the lambda function checks whether the input i is greater than 25.\n",
    "\n",
    "So, you can interpret this as follows: **gt25 is a function that, when given a value i, returns True if i is greater than 25 and False otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7afec4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:53.252540Z",
     "iopub.status.busy": "2023-11-28T07:27:53.252049Z",
     "iopub.status.idle": "2023-11-28T07:27:53.258265Z",
     "shell.execute_reply": "2023-11-28T07:27:53.257012Z"
    },
    "papermill": {
     "duration": 0.038747,
     "end_time": "2023-11-28T07:27:53.267418",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.228671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "gt25 = lambda i: i > 25\n",
    "\n",
    "print(gt25(4))\n",
    "print(gt25(51))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1933048",
   "metadata": {
    "papermill": {
     "duration": 0.029378,
     "end_time": "2023-11-28T07:27:53.327470",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.298092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Shuffling the RDD\n",
    "\n",
    "`l = list(range(100))`: Creates a list containing numbers from 0 to 99. This is a simple way to generate a list with sequential integers.\n",
    "\n",
    "`shuffle(l)`: Uses the shuffle function from the random module to randomly shuffle the elements of the list l. This step is intended to randomize the order of the numbers in the list.\n",
    "\n",
    "`sc.parallelize(l)`: Creates an RDD named rdd from the shuffled list l. The parallelize method of the SparkContext (sc) is used to distribute the elements of the list across the nodes of the Spark cluster. This RDD can then be used for parallel processing using Spark.\n",
    "\n",
    "So, after running this code, the _rdd_ variable holds a distributed representation of the shuffled list, and the elements are distributed across the nodes of the Spark cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75090be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:53.392510Z",
     "iopub.status.busy": "2023-11-28T07:27:53.391145Z",
     "iopub.status.idle": "2023-11-28T07:27:53.402348Z",
     "shell.execute_reply": "2023-11-28T07:27:53.400960Z"
    },
    "papermill": {
     "duration": 0.047216,
     "end_time": "2023-11-28T07:27:53.405712",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.358496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Create a list containing numbers from 0 to 99\n",
    "l = list(range(100))\n",
    "\n",
    "# Shuffle the list randomly\n",
    "shuffle(l)\n",
    "\n",
    "# Create an RDD (Resilient Distributed Dataset) from the shuffled list\n",
    "rdd = sc.parallelize(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3ecac",
   "metadata": {
    "papermill": {
     "duration": 0.022209,
     "end_time": "2023-11-28T07:27:53.458405",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.436196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Filtering values from our list which are equals or less than 50 by applying the “gt50” function to the list using the “filter” function. \n",
    "**Note that by calling the “collect” function, all elements are returned to the Apache Spark Driver. This is not a good idea for BigData, please use “.sample(10,0.1).collect()” or “take(n)” instead. beacuse collect can be resource-intensive, especially if the RDD is large, as it brings all the data back to the driver program.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9a97fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:53.507819Z",
     "iopub.status.busy": "2023-11-28T07:27:53.506581Z",
     "iopub.status.idle": "2023-11-28T07:27:53.895776Z",
     "shell.execute_reply": "2023-11-28T07:27:53.894559Z"
    },
    "papermill": {
     "duration": 0.418995,
     "end_time": "2023-11-28T07:27:53.899329",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.480334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 72, 99, 87, 68, 89, 90, 63, 96, 74]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd.filter(gt50).collect() #we can use collect but following the best practice\n",
    "rdd.filter(gt50).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be5888",
   "metadata": {
    "papermill": {
     "duration": 0.029551,
     "end_time": "2023-11-28T07:27:53.950541",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.920990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "sample(10, 0.1): This is another transformation, specifically the sample transformation. It is used to create a random sample of the elements in the RDD. The parameters are:\n",
    "\n",
    "`10`: The number of elements to include in the sample.\n",
    "`0.1`: The sampling fraction, representing the probability that each element is included in the sample. In this case, it's set to 0.1, meaning there's a 10% chance of including each element in the sample.\n",
    "\n",
    "Note that the result of this operation doesn't directly give you a collection of elements; instead, it returns a new RDD that represents the sampled subset of the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c462a7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:54.034349Z",
     "iopub.status.busy": "2023-11-28T07:27:54.033732Z",
     "iopub.status.idle": "2023-11-28T07:27:55.092123Z",
     "shell.execute_reply": "2023-11-28T07:27:55.091022Z"
    },
    "papermill": {
     "duration": 1.122021,
     "end_time": "2023-11-28T07:27:55.112369",
     "exception": false,
     "start_time": "2023-11-28T07:27:53.990348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 69]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = rdd.filter(gt50).sample(10, 0.1).take(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f60ba16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:55.165032Z",
     "iopub.status.busy": "2023-11-28T07:27:55.164609Z",
     "iopub.status.idle": "2023-11-28T07:27:55.182112Z",
     "shell.execute_reply": "2023-11-28T07:27:55.180880Z"
    },
    "papermill": {
     "duration": 0.043607,
     "end_time": "2023-11-28T07:27:55.184788",
     "exception": false,
     "start_time": "2023-11-28T07:27:55.141181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying with sample api\n",
    "rdd.filter(gt50).sample(10,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6805b744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:55.232269Z",
     "iopub.status.busy": "2023-11-28T07:27:55.230798Z",
     "iopub.status.idle": "2023-11-28T07:27:55.534780Z",
     "shell.execute_reply": "2023-11-28T07:27:55.533399Z"
    },
    "papermill": {
     "duration": 0.3306,
     "end_time": "2023-11-28T07:27:55.538005",
     "exception": false,
     "start_time": "2023-11-28T07:27:55.207405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 72, 99, 87, 68, 89, 90, 63, 96, 74]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda i: i > 50).take(10) #running the same code but with a lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f4859",
   "metadata": {
    "papermill": {
     "duration": 0.023835,
     "end_time": "2023-11-28T07:27:55.583879",
     "exception": false,
     "start_time": "2023-11-28T07:27:55.560044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s consider the same list of integers. Now we want to compute the sum for elements in that list which are greater than 50 but less than 75. Please implement the missing parts. \n",
    "\n",
    "`rdd.filter(lambda x: $$).filter(lambda x: $$).$$()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eae465c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T07:27:55.632434Z",
     "iopub.status.busy": "2023-11-28T07:27:55.631932Z",
     "iopub.status.idle": "2023-11-28T07:27:56.230246Z",
     "shell.execute_reply": "2023-11-28T07:27:56.229052Z"
    },
    "papermill": {
     "duration": 0.626035,
     "end_time": "2023-11-28T07:27:56.233092",
     "exception": false,
     "start_time": "2023-11-28T07:27:55.607057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 72, 68, 63, 74, 58, 66, 69, 59, 52]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x > 50 and x < 75).take(10) # shortened the code but doing that in one filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed31e5",
   "metadata": {
    "papermill": {
     "duration": 0.021731,
     "end_time": "2023-11-28T07:27:56.277124",
     "exception": false,
     "start_time": "2023-11-28T07:27:56.255393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.025326,
   "end_time": "2023-11-28T07:27:58.920405",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-28T07:26:44.895079",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
